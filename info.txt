Apache Beam
    - Unified Programming Lanuage
    - used for portable big data processing, supported by frameworks like Psark, Flink, Apex, Cloud Dataflow, etc.
    - Beam => Batch + Stream
    - first introduced in 2016
    - github repo => https://github.com/apache/beam

Supported SDKs
    1. Java
    2. Python
    3. Go
    4. Other

Runners/Executors
    1. Spark
    2. Flink
    3. Apex
    4. Dataflow
    5. Samza
    6. Other

Flow of Beam Programming Model
    Input => PCollection => Output

    Input = text files, log files, database, stream(kafka/pub-sub)
    Output = text file, in memory,. hdfs, gfs, stream, (kafka/pub-sub)
    Transform = Data tranformation methods

Basic Terminologies
    1. Pipeline = A pipeline encapsulates entire data processing task, from start to finish. Includes reading data, transforming that data, and writing output data.
    2. Pcollection = A PCollection is equivalent to RDD in Spark. It represents a distributed dataset that our Beam pipeline operates on.
    3. PTransform = A PTransform represents a data processing operation, or a step, in our pipeline. Ex. ParDo, filter, flatten, combine, etc.

PCollection Charactestics
    1. Immutability = PCollections are immutable in nature. Applying a transf0rmation on PCollection results in creation of new PCollection.
    2. Element Type = The elements in a PCollection may be of any type, but all must be of same type.
    3. Operation Type = PCollections does not support grained operations. We cannot apply transformations on some spe3cific elements in a PCollection.
    4. TimeStamps = Each element in a PCollection has an associated timestamp with it.
        Unbounded PCollections => Source assign the timestamp.
        Bounded PCollections => Every element is set to same timestamp.

Read Transform
    1. ReadFromText() => parses textfile as newline delimited elements i.e., it reads the file line by line and every line is single element in PCollection. (https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.io.textio.html#:~:text=A%20PTransform%20for%20reading%20text%20files.%20Parses%20a%20text%20file)

    2. ReadFrmAvro() => Used to read Avro File (https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.io.avroio.html)

    3. ReadFromParquet() => Used to read parquet file (https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.io.parquetio.html#:~:text=Initializes%20ReadFromParquet.%20Uses%20source%20_ParquetSource%20to%20read%20a%20set%20of)

    4. ReadFromTFRecord() => Used to read TensorFlow records.(https://beam.apache.org/releases/pydoc/2.36.0/apache_beam.io.tfrecordio.html#:~:text=apache_beam.io.tfrecordio%20module%20%C2%B6.%20TFRecord%20sources%20and%20sinks.%20class)

    5. ReadFromPubSub() => Used to read messages from Google PubSub Service (https://beam.apache.org/releases/pydoc/2.5.0/apache_beam.io.gcp.pubsub.html#:~:text=Beam%20users%20should%20not%20directly%20construct%20PubsubMessages.%20class)

Write Transform
    1. WriteToText() => Write each element f PCollection as a single line in the output file. (https://beam.apache.org/releases/pydoc/2.2.0/apache_beam.io.textio.html#:~:text=A%20PTransform%20for%20reading%20a%20PCollection%20of%20text%20files.%20Reads)

    2. WriteToAvro() => Writes each element of PCollection to Avro File (https://beam.apache.org/releases/pydoc/2.32.0/apache_beam.io.avroio.html#:~:text=Uses%20source%20_AvroSource%20to%20read%20a%20set%20of%20Avro%20files)

    3. WriteToParquet() => Writes each element of the PCollection to parquet file. (https://beam.apache.org/releases/pydoc/2.25.0/apache_beam.io.parquetio.html#:~:text=WriteToParquet%20(filename,%20pyarrow.%20schema%20([('name',%20pyarrow.%20binary%20()),)

    4. WriteToTFRecord() => Writes each element of the PCollection to Tensorflow records. (https://beam.apache.org/releases/pydoc/2.36.0/apache_beam.io.tfrecordio.html#:~:text=Transform%20for%20reading%20TFRecord%20sources.%20Initialize%20a%20ReadFromTFRecord%20transform.)

    5. WriteToPubSub() => Writes each element of the PCollection to Google cloud PubSub Service (https://beam.apache.org/releases/pydoc/2.8.0/_modules/apache_beam/io/gcp/pubsub.html#:~:text=class%20WriteToPubSub%20(PTransform):%20%22%22%22A%20%60%60PTransform%60%60%20for%20writing%20messages%20to)

Transforms
    1. Map => Applies a simple 1-to-1 mapping function over each element in the collection.(https://beam.apache.org/documentation/transforms/python/elementwise/map/)

    2. FlatMap => Applies a simple 1-to-many mapping function over each element in the collection. The many elements are flattened into the resulting collection. (https://beam.apache.org/documentation/transforms/python/elementwise/flatmap/)

    3. Filter => Given a predicate, filter out all elements that donâ€™t satisfy that predicate. May also be used to filter based on an inequality with a given value based on the comparison ordering of the element.(https://beam.apache.org/documentation/transforms/python/elementwise/filter/)

    3. Keys => Takes a collection of key-value pairs and returns the key of each element. (https://beam.apache.org/documentation/transforms/python/elementwise/keys/)

    4. KvSwap => Takes a collection of key-value pairs and returns a collection of key-value pairs which has each key and value swapped. (https://beam.apache.org/documentation/transforms/python/elementwise/kvswap/)

    5. ParDo => A transform for generic parallel processing. A ParDo transform considers each element in the input PCollection, performs some processing function (your user code) on that element, and emits zero or more elements to an output PCollection. (https://beam.apache.org/documentation/transforms/python/elementwise/pardo/)

    6. Partition => Separates elements in a collection into multiple output collections. The partitioning function contains the logic that determines how to separate the elements of the input collection into each resulting partition output collection. The number of partitions must be determined at graph construction time. You cannot determine the number of partitions in mid-pipeline.(https://beam.apache.org/documentation/transforms/python/elementwise/partition/)

